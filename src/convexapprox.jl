defaultpenalty() = :l1
defaultpenalty2D() = :l2
defaultseg() = 5
defaultplanes() = 4

"""
    convex_linearization(x, z, optimizer; kwargs...)

Computes a piecewise linear function that approximates the measurements given by `x` and `z`.

# Arguments
- `method::Symbol:=fit`: the method used for approximation
- `dimensions::Integer:=2`: the number of dimensions of the function domain
- `nseg::Integer=5`: the number of segments to use 
- `nplanes::Integer=4`: the number of planes to use in 2D PWL functions
- `strict::Symbol=:none`: defines it is a general approximation, or an overestimation or underestimation
- `pen::Symbol=:l1`: the metric used to measure deviation


"""
function convex_linearization(x, z, optimizer; kwargs...)
    defaults = (;method=:fit, dimensions=1)
    options = merge(defaults, kwargs)

    method = options.method
    dimensions = options.dimensions

    if dimensions == 1
        @assert(length(x) == length(z))
        if method == :fit
            return convex_linearization_fit(x, z, optimizer; kwargs...)
        elseif method == :ipol
            return convex_linearization_ipol(x, z, optimizer; kwargs...)
        else
            error("Unrecognized method $method")
        end
    elseif dimensions >= 2
        if method == :fit
            return convex_ND_linearization_fit(x, z, optimizer; kwargs...)
        end    
    else
        error("Unrecognized number of dimensions $dimensions")
    end    
end

function conv_linear_big_M(x, z)
    N = length(x)
    cрхЅ╦брхЌ = (z[N] -z[N-1])/(x[N]-x[N-1])
    return  2 * cрхЅ╦брхЌ * (last(x) - first(x)) - maximum(z)
end

#convex_linearization(x, z, optimizer; kwargs...)  = 
#    convex_linearization([xx for xx in x], [zz for zz in z], optimizer; kwargs...)

function convex_linearization_fit(x::Vector, z::Vector, optimizer; kwargs...)
  
    defaults = (nseg=defaultseg(), pen=defaultpenalty(), strict=false, start_origin=false, show_res=false)
    options = merge(defaults, kwargs)
  
    N = length(x)
    ­ЮњЕ = 1:N 
    ­Юњд = 1:options.nseg
    
    MрхЄРЂ▒рхЇ =  conv_linear_big_M(x,z)
    
    m = Model()
    @variable(m, ­ЮЉД╠ѓ[­ЮњЕ]) 
    @variable(m, ­ЮЉљ[­Юњд])
    @variable(m, ­ЮЉЉ[­Юњд]) 
    @variable(m, ­ЮЉб[­ЮњЕ,­Юњд], Bin)

    if options.pen == :l2 
        @objective(m, Min, sum((z[i] - ­ЮЉД╠ѓ[i])^2 for i Рѕѕ ­ЮњЕ))
    elseif options.pen == :max
        ­ЮЉА = @variable(m)
        @objective(m, Min, ­ЮЉА)
        for i Рѕѕ ­ЮњЕ
            @constraint(m,  ­ЮЉА РЅЦ (z[i] - ­ЮЉД╠ѓ[i]) )
            @constraint(m,  ­ЮЉА РЅЦ (­ЮЉД╠ѓ[i] - z[i]) )
        end
    elseif options.pen == :l1
        ­ЮЉА = @variable(m, [­ЮњЕ])
        @objective(m, Min, sum(­ЮЉА))
        for i Рѕѕ ­ЮњЕ
            @constraint(m,  ­ЮЉА[i] РЅЦ (z[i] - ­ЮЉД╠ѓ[i]) )
            @constraint(m,  ­ЮЉА[i] РЅЦ (­ЮЉД╠ѓ[i] - z[i]) )
        end
    else
        error("Unrecognized/unsupported penalty type $(options.pen)")
    end

    for i Рѕѕ ­ЮњЕ, k Рѕѕ ­Юњд 
        @constraint(m, ­ЮЉД╠ѓ[i] РЅЦ ­ЮЉљ[k] * x[i] + ­ЮЉЉ[k])
        @constraint(m, ­ЮЉД╠ѓ[i] РЅц ­ЮЉљ[k] * x[i] + ­ЮЉЉ[k] + MрхЄРЂ▒рхЇ * (1-­ЮЉб[i,k]))
    end

    if options.strict
        for i Рѕѕ ­ЮњЕ, k Рѕѕ ­Юњд 
            @constraint(m, z[i] РЅЦ ­ЮЉљ[k] * x[i] + ­ЮЉЉ[k])   
        end
    end

    if options.start_origin
        @constraint(m,­ЮЉЉ[1] == 0.0)
    end

    for i Рѕѕ ­ЮњЕ
        @constraint(m, sum(­ЮЉб[i,k] for k Рѕѕ ­Юњд) РЅЦ 1)
    end

    for k Рѕѕ ­Юњд 
        if k > 1
            @constraint(m, ­ЮЉљ[k-1] РЅц ­ЮЉљ[k])
        end
    end


    set_optimizer(m,optimizer)
    optimize!(m)

    if termination_status(m) != MOI.OPTIMAL
        error("Optimization failed")
    end

    if options.show_res
        println("Optimize succeed for $(options.pen)")
        val = objective_value(m)
        println("Objective value = $val")
    end
    
    ­ЮЉљр┤╝рхќрхЌ = value.(­ЮЉљ)
    ­ЮЉЉр┤╝рхќрхЌ = value.(­ЮЉЉ) 

    return ConvexPWLFunction([­ЮЉљр┤╝рхќрхЌ[k] for k Рѕѕ ­Юњд], [­ЮЉЉр┤╝рхќрхЌ[k] for k Рѕѕ ­Юњд], minimum(x), maximum(x))
end

function convex_linearization(f::Function, xmin, xmax, optimizer; kwargs...)
    @assert(xmin < xmax)

    defaults = (nsample=10, nseg=defaultseg()) 
    options = merge(defaults, kwargs)

    samples = max(options.nsample, 3*options.nseg)

    step = (xmax - xmin) / samples
    x = [i for i in xmin:step:xmax]
    y = [f(xx) for xx in x]
    return convex_linearization(x, y, optimizer; kwargs...)
end

function concave_linearization(x, z, optimizer; kwargs...)
    if (size(x,2) < 2) && (length(first(x)) < 2)
        return ConcavePWLFunction(convex_linearization(x, -z, optimizer; kwargs...))
    else
        return ConcavePWLFunctionND(convex_linearization(x, -z, optimizer; kwargs...))
    end
end

function concave_linearization(f::Function, xmin, xmax, optimizer; kwargs...)
    if (size(x,2) < 2) && (length(first(X)) < 2)
        return ConcavePWLFunction(convex_linearization(x -> -f(x), xmin, xmax, optimizer; kwargs...))
    else
        return ConcavePWLFunctionND(convex_linearization(x -> -f(x), xmin, xmax, optimizer; kwargs...))
    end
end


function convexify(pwl::PWLFunction, optimizer)
    N = length(pwl.x)
    ­ЮњЕ = 1: N
    x = pwl.x
    z = pwl.z

    m = Model()

    ╬┤РЂ║ = @variable(m, [­ЮњЕ], lower_bound=0)
    ╬┤РЂ╗ = @variable(m, [­ЮњЕ], lower_bound=0)

    @objective(m, Min, sum(╬┤РЂ║[i] + ╬┤РЂ╗[i] for i Рѕѕ ­ЮњЕ))
    for i=2:N-1
        @constraint(m, (z[i] + ╬┤РЂ║[i] - ╬┤РЂ╗[i] - z[i-1] - ╬┤РЂ║[i-1] + ╬┤РЂ╗[i-1]) / 
            (x[i] -x[i-1]) <= (z[i+1] + ╬┤РЂ║[i+1] - ╬┤РЂ╗[i+1] - z[i] - ╬┤РЂ║[i] + ╬┤РЂ╗[i]) / (x[i+1] -x[i]))
    end

    set_optimizer(m,optimizer)
    optimize!(m)
    if termination_status(m) != MOI.OPTIMAL
        error("Optimization failed")
    end

    dp = value.(╬┤РЂ║)
    dn = value.(╬┤РЂ╗)

    return ConvexPWLFunction(x, [z[i] + dp[i] - dn[i] for i Рѕѕ ­ЮњЕ]) 
end

convexify(x, z, optimizer) =
    convexify(PWLFunction(x,z), optimizer)

function interpolatepw(x, z, optimizer; kwargs...)
    @assert(length(x) == length(z))

    defaults = (nseg=defaultseg(), pen=defaultpenalty())
    options = merge(defaults, kwargs)
   
    N = length(x)
    ­ЮњЕ = 1:N 

    # Find slopes
    c = [(z[j] -z[i]) / (x[j]-x[i])  for i Рѕѕ ­ЮњЕ, j Рѕѕ ­ЮњЕ]

    # Calculate penalties
    if options.pen == :l1
        p = [(i < j ? sum(abs(c[i,j] * (x[k] - x[i]) + z[i] - z[k]) for k Рѕѕ i:j) : 0) for i Рѕѕ ­ЮњЕ, j Рѕѕ ­ЮњЕ]
    elseif options.pen == :l2
        p = [(i < j ? sum((c[i,j] * (x[k] - x[i]) + z[i] - z[k])^2 for k Рѕѕ i:j) : 0) for i Рѕѕ ­ЮњЕ, j Рѕѕ ­ЮњЕ]
    elseif options.pen == :max
        p = [(i < j ? maximum(abs(c[i,j] * (x[k] - x[i]) + z[i] - z[k]) for k Рѕѕ i:j) : 0) for i Рѕѕ ­ЮњЕ, j Рѕѕ ­ЮњЕ]
    else
        error("Unrecognized/unsupported penalty $(options.pen)")
    end
    
    m = Model()
    @variable(m, ­ЮЉб[­ЮњЕ,­ЮњЕ], Bin)

    # Minimize total penalty
    @objective(m, Min, sum(p[i,j] * ­ЮЉб[i,j] for i Рѕѕ ­ЮњЕ, j Рѕѕ ­ЮњЕ))

    # Number of line segments in interpolant
    @constraint(m, sum(­ЮЉб[i,j] for i Рѕѕ ­ЮњЕ, j Рѕѕ ­ЮњЕ) == options.nseg )

    # Only forward segments allowed
    for i Рѕѕ ­ЮњЕ, j Рѕѕ ­ЮњЕ 
        if i >= j 
            @constraint(m, ­ЮЉб[i,j] == 0)
        end
    end 

    # Path modelling
    for j Рѕѕ ­ЮњЕ
        @constraint(m, (j > 1 ? sum(­ЮЉб[i,j] for i Рѕѕ ­ЮњЕ) : 1) == (j < N ? sum(­ЮЉб[j,i] for i Рѕѕ ­ЮњЕ ) : 1))
    end

    set_optimizer(m,optimizer)
    optimize!(m)
    if termination_status(m) != MOI.OPTIMAL
        error("Optimization failed")
    end
    
    ­ЮЉбр┤╝рхќрхЌ = value.(m[:­ЮЉб])
    xр┤╝рхќрхЌ = collect(x[i] for i Рѕѕ ­ЮњЕ, j Рѕѕ ­ЮњЕ if ­ЮЉбр┤╝рхќрхЌ[i,j] == 1)
    push!(xр┤╝рхќрхЌ, x[N])
    zр┤╝рхќрхЌ = collect(z[i] for i Рѕѕ ­ЮњЕ, j Рѕѕ ­ЮњЕ if ­ЮЉбр┤╝рхќрхЌ[i,j] == 1)
    push!(zр┤╝рхќрхЌ, z[N])

    return PWLFunction(xр┤╝рхќрхЌ, zр┤╝рхќрхЌ)
end

convex_linearization_ipol(x, z, optimizer; kwargs...) = 
    convexify(interpolatepw(x, z, optimizer; kwargs...),optimizer)

function mat2tuples(x::Matrix)
    return collect(Tuple(x'[:,i]) for i in 1:size(x',2))
end
# mat2tuples2(x::Matrix) = ((Tuple(x[i,:]) for i in 1:size(x,1)))

function tuples2mat(­ЮњФ::Vector{Tuple{Float64, Float64}})
   return reduce(hcat, getindex.(­ЮњФ,i) for i in eachindex(­ЮњФ[1]))
end

function convex_ND_linearization_fit(x::Matrix{Float64}, z, optimizer; kwargs...)
    return convex_ND_linearization_fit(mat2tuples(x), z, optimizer; kwargs...)
end

function convND_linear_big_M(­ЮњФ::Vector{Tuple{Float64, Float64}}, z)
    return convND_linear_big_M(tuples2mat(­ЮњФ),z)
end

function convND_linear_big_M(x::Matrix{Float64}, z)
    ## TODO: calculate a tighter value for the big-M    
    return 2*maximum(z)
end

function convex_ND_linearization_fit(­ЮњФ, z, optimizer; kwargs...)

    defaults = (nsegs=defaultseg(), nplanes=defaultplanes(), pen=defaultpenalty2D(), strict=:none, show_res=false)
    options = merge(defaults, kwargs)

    zрхќ = Dict(zip(­ЮњФ, z))
    ­Юњд = 1:options.nplanes
    РёљРѓџ = 1:length(­ЮњФ[1])    

    MрхЄРЂ▒рхЇ = convND_linear_big_M(­ЮњФ, z) 

    m = Model()
    @variable(m, ­ЮЉД╠ѓ[­ЮњФ])
    @variable(m, a[РёљРѓџ, ­Юњд])
    @variable(m, b[­Юњд])

    @variable(m, ­ЮЉб[­ЮњФ, ­Юњд], Bin)

    if options.pen == :l2 
        @objective(m, Min, sum((zрхќ[p] - ­ЮЉД╠ѓ[p])^2 for p Рѕѕ ­ЮњФ))
    elseif options.pen == :max
        ­ЮЉА = @variable(m)
        @objective(m, Min, ­ЮЉА)
        for p Рѕѕ ­ЮњФ
            @constraint(m,  ­ЮЉА РЅЦ (zрхќ[p] - ­ЮЉД╠ѓ[p]) )
            @constraint(m,  ­ЮЉА РЅЦ (­ЮЉД╠ѓ[p] - zрхќ[p]) )
        end
    elseif options.pen == :l1
        ­ЮЉА = @variable(m, [­ЮњФ])
        @objective(m, Min, sum(­ЮЉА))
        for p Рѕѕ ­ЮњФ
            @constraint(m,  ­ЮЉА[p] РЅЦ (zрхќ[p] - ­ЮЉД╠ѓ[p]) )
            @constraint(m,  ­ЮЉА[p] РЅЦ (­ЮЉД╠ѓ[p] - zрхќ[p]) )
        end
    else
        error("Unrecognized/unsupported penalty type $(options.pen)")
    end
     
    for p Рѕѕ ­ЮњФ, k Рѕѕ ­Юњд         
        @constraint(m, ­ЮЉД╠ѓ[p] РЅЦ sum(a[j,k] * p[j] for j in РёљРѓџ) + b[k])
        @constraint(m, ­ЮЉД╠ѓ[p] РЅц sum(a[j,k] * p[j] for j in РёљРѓџ) + b[k] + MрхЄРЂ▒рхЇ * (1-­ЮЉб[p,k]))                
    end

    if options.strict == :above
        for p Рѕѕ ­ЮњФ, k Рѕѕ ­Юњд 
            @constraint(m, zрхќ[p] РЅЦ sum(a[j,k] * p[j] for j in РёљРѓџ) + b[k]) 
        end
    elseif options.strict == :below
        for p Рѕѕ ­ЮњФ, k Рѕѕ ­Юњд 
            @constraint(m, zрхќ[p] РЅц sum(a[j,k] * p[j] for j in РёљРѓџ) + b[k]) 
        end
    end
    
    for p Рѕѕ ­ЮњФ
        @constraint(m, sum(­ЮЉб[p,k] for k Рѕѕ ­Юњд) РЅЦ 1)
    end    
    
    set_optimizer(m,optimizer)
    optimize!(m)

    if termination_status(m) != MOI.OPTIMAL
        error("Optimization failed")
    end

    if options.show_res
        println("Optimize succeed for $(options.pen)")
        val = objective_value(m)
        println("Objective value = $val")
    end   
    
    aр┤╝рхќрхЌ = value.(a)
    bр┤╝рхќрхЌ = value.(b)    
    
    return ConvexPWLFunctionND(collect([Tuple(aр┤╝рхќрхЌ.data[:,k]) for k Рѕѕ ­Юњд]),  [bр┤╝рхќрхЌ[k] for k Рѕѕ ­Юњд])
    ##TODO: how to recover the data points from the coefficients?  Check package Polyhedra.    
end    

