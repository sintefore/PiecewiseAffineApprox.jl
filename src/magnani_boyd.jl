
# Approximation error using the value of the pwl function
# in the given points. Support for multiple metrics (l1, l2, max).
function _approx_error(X::Matrix, z::Vector, pwl::PWLFunc, penalty = :l1)
    err = 0.0
    for (i, xÌ„) in enumerate(eachcol(X))
        v = evaluate(pwl, xÌ„)

        if penalty == :l1
            err += abs(v - z[i])
        elseif penalty == :l2 || penalty == :rms
            err += (v - z[i])^2
        elseif penalty == :max
            err = max(err, abs(v - z[i]))
        end
    end

    if penalty == :rms
        err = err / size(X, 2)
    end

    if penalty == :l2 || penalty == :rms
        err = sqrt(err)
    end

    return err
end


function _convex_linearization_mb_single(X::Matrix, z::Vector, K, láµáµƒË£, penalty, optimizer, strict)
    ğ’« = _random_partition(X, K)
    pwl = _refine_partition(X, z, ğ’«, láµáµƒË£, penalty, optimizer, strict)
    e = _approx_error(X, z, pwl, penalty)
    return e=>pwl
end

# Finds a pwl convex approximation for the provided data
# X is a matrix with a column for each data point and z is a vector with the 
# corresponding function values
function _convex_linearization_mb(X::Matrix, z::Vector; kwargs...)
    @assert(size(X, 2) == length(z))

    defaults = (
        planes = defaultplanes(),
        pen = defaultpenalty(),
        trials = 20,
        itlim = 50,
        strict = :none,
    )
    options = merge(defaults, kwargs)

    Náµ—Ê³ = options.trials     # Number of trials
    láµáµƒË£ = options.itlim    # Iteration limit
    K = options.planes
    penalty = options.pen
    strict = options.strict
    optimizer = options.optimizer

    @info "Starting heuristic search "
    approxes = fetch.(@spawn _convex_linearization_mb_single(X, z, K, láµáµƒË£, penalty, optimizer, strict) for i âˆˆ 1:Náµ—Ê³)
    min_error, pwl_best = argmin(first, approxes)
    
    @info "Terminating search - best approximation error = $(min_error) ($penalty)"
    return pwl_best
end

# Euclidean distance between two points
_dist(x, y) = sqrt(sum((x - y) .^ 2))

# Create a random partition of the points into K sets
# by generating K random points and group each data point to the
# closest of these random points.
function _random_partition(X, K)
    Î¼ = vec(mean(X, dims = 2))
    ÏƒÂ² = cov(X, dims = 2)
    n = MvNormal(Î¼, ÏƒÂ²)
    P = rand(n, K)

    ğ’« = [[] for j âˆˆ 1:K]
    for (i, x) in enumerate(eachcol(X))
        # Find the nearest point amongst the p's
        jmin = argmin(_dist(x, P[:, j]) for j âˆˆ 1:K)
        push!(ğ’«[jmin], i)
    end
    return ğ’«
end

# Improve a partition by fitting a hyperplane for all points 
# belonging to each subset of the partition and then 
# updating the partition such that each point is associated with the
# plane being active at the point. 
#
# The process terminates after a given number of iterations returning
# the pwl function corresponding to the final partition.
function _refine_partition(X, z, ğ’«, láµáµƒË£, penalty, optimizer, strict)
    D = size(X, 1)
    pwl = nothing
    for it âˆˆ 1:láµáµƒË£
        pwl = PWLFunc{Convex,D}()
        for p in ğ’«
            if length(p) > 0
                xÌ„ = X[:, p]
                zÌ„ = z[p]
                a, b = _local_fit(xÌ„, zÌ„, penalty, optimizer, strict)
                _addplane!(pwl, a, b)
            end
        end
        ğ’«â¿áµ‰Ê· = _update_partition(X, pwl)
        if ğ’«â¿áµ‰Ê· == ğ’«
            break
        end
        ğ’« = ğ’«â¿áµ‰Ê·
    end

    return pwl
end

# Finds the hypeplane best fitting the given subset of points 
# using the given penalty and possible restrictions on whether
# the plane should be strictly above or below the points
function _local_fit(XÌ„, zÌ„, penalty, optimizer, strict)
    M, N = size(XÌ„)

    # Create an optimization model to find the best a and b such that  ax + b â‰ˆ z
    m = Model()
    
    if Threads.nthreads() > 1 
        # Use threading for individual problems, avoid oversubscribing by limiting threads for each problem
        MOI.set(m, MOI.NumberOfThreads(), 1)
    end

    @variable(m, a[1:M])
    @variable(m, b)
    @variable(m, zÌ‚[1:N])

    for i âˆˆ 1:N
        @constraint(m, zÌ‚[i] == sum(a[j] * XÌ„[j, i] for j âˆˆ 1:M) + b)
    end

    if strict == :above
        @constraint(m, zÌ‚ .â‰¥ zÌ„)
    elseif strict == :below
        @constraint(m, zÌ‚ .â‰¤ zÌ„)
    end

    obj = AffExpr()
    if penalty == :l2 || penalty == :rms
        obj = sum((zÌ„[i] - zÌ‚[i])^2 for i âˆˆ 1:N)
    elseif penalty == :max
        @variable(m, t)
        obj = t
        for i âˆˆ 1:N
            @constraint(m, t â‰¥ (zÌ„[i] - zÌ‚[i]))
            @constraint(m, t â‰¥ (zÌ‚[i] - zÌ„[i]))
        end
    elseif penalty == :l1
        @variable(m, t[1:N])
        obj = sum(t[i] for i âˆˆ 1:N)
        for i âˆˆ 1:N
            @constraint(m, t[i] â‰¥ (zÌ„[i] - zÌ‚[i]))
            @constraint(m, t[i] â‰¥ (zÌ‚[i] - zÌ„[i]))
        end
    else
        error("Unrecognized/unsupported penalty type $(penalty)")
    end

    # TODO: consider adding regularization term

    @objective(m, Min, obj)

    set_optimizer(m, optimizer)
    set_silent(m)
    optimize!(m)

    if termination_status(m) != MOI.OPTIMAL
        error("Optimization failed")
    end

    aÌ„ = JuMP.value.(a)
    bÌ„ = JuMP.value.(b)

    return aÌ„, bÌ„
end

# The hyperplane being active at the point x for a convex pwl function 
function _active(pwl::PWLFunc{Convex,D}, x) where {D}
    return argmax(collect(evaluate(p, x) for p âˆˆ pwl.planes))
end

# Creating an updated partition by associating each data point to
# the hyperplane being active for the data point
function _update_partition(X, pwl)
    ğ’« = [[] for j âˆˆ 1:_planes(pwl)]
    for (i, x) in enumerate(eachcol(X))
        push!(ğ’«[_active(pwl, x)], i)
    end
    return ğ’«
end
